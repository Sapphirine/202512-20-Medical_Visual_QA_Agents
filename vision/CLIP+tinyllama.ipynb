{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb7c3c6-8e7b-4bdc-9d76-189667f96c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "CLIP vision loaded. hidden_size = 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyLLaMA loaded. hidden_size = 2048\n",
      "Projector loaded on cuda\n",
      "Train set: 19654  Test set: 6719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 15:  69%|██████▊   | 13497/19654 [11:25<05:14, 19.55it/s]/usr/local/lib/python3.12/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n",
      "Epoch 1 / 15: 100%|██████████| 19654/19654 [16:42<00:00, 19.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 1] train loss = 1.1457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] val F1 = 0.0523, val EM = 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 / 15: 100%|██████████| 19654/19654 [16:57<00:00, 19.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 2] train loss = 1.0456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] val F1 = 0.3245, val EM = 0.2722\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 / 15: 100%|██████████| 19654/19654 [16:52<00:00, 19.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 3] train loss = 0.9545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 / 15: 100%|██████████| 19654/19654 [16:03<00:00, 20.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 12] train loss = 0.5515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  35%|███▍      | 2322/6719 [13:40<25:29,  2.88it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    CLIPVisionModel,\n",
    "    CLIPImageProcessor,\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "clip_name = \"openai/clip-vit-base-patch32\"\n",
    "clip_vision = CLIPVisionModel.from_pretrained(clip_name).to(device)\n",
    "clip_processor = CLIPImageProcessor.from_pretrained(clip_name)\n",
    "\n",
    "for p in clip_vision.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "vision_width = clip_vision.config.hidden_size\n",
    "print(\"CLIP vision loaded. hidden_size =\", vision_width)\n",
    "\n",
    "llama_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama_name)\n",
    "llama = AutoModelForCausalLM.from_pretrained(\n",
    "    llama_name,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(device)\n",
    "\n",
    "for p in llama.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "llama.train()\n",
    "lm_hidden_size = llama.config.hidden_size\n",
    "print(\"TinyLLaMA loaded. hidden_size =\", lm_hidden_size)\n",
    "\n",
    "class Projector(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "projector = Projector(vision_width, lm_hidden_size).to(device)\n",
    "print(\"Projector loaded on\", device)\n",
    "\n",
    "def normalize(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s)\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "def f1(pred, gt):\n",
    "    pt = normalize(pred).split()\n",
    "    gt = normalize(gt).split()\n",
    "    if len(pt) == 0 or len(gt) == 0:\n",
    "        return float(pt == gt)\n",
    "    common = set(pt) & set(gt)\n",
    "    if not common:\n",
    "        return 0.0\n",
    "    p = len(common) / len(pt)\n",
    "    r = len(common) / len(gt)\n",
    "    return 2 * p * r / (p + r)\n",
    "\n",
    "def em(pred, gt):\n",
    "    return float(normalize(pred) == normalize(gt))\n",
    "\n",
    "train_ds = load_dataset(\"flaviagiammarino/path-vqa\", split=\"train\")\n",
    "test_ds  = load_dataset(\"flaviagiammarino/path-vqa\", split=\"test\")\n",
    "\n",
    "MAX_TRAIN_SAMPLES = None\n",
    "MAX_TEST_SAMPLES  = None\n",
    "\n",
    "if MAX_TRAIN_SAMPLES:\n",
    "    train_ds = train_ds.select(range(MAX_TRAIN_SAMPLES))\n",
    "if MAX_TEST_SAMPLES:\n",
    "    test_ds = test_ds.select(range(MAX_TEST_SAMPLES))\n",
    "\n",
    "print(\"Train set:\", len(train_ds), \" Test set:\", len(test_ds))\n",
    "\n",
    "def build_inputs_and_labels(question, answer):\n",
    "    prompt = f\"Question: {question}\\nAnswer:\"\n",
    "    full   = f\"{prompt} {answer}\"\n",
    "\n",
    "    tok_full   = tokenizer(full, return_tensors=\"pt\")\n",
    "    tok_prompt = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    input_ids_full = tok_full.input_ids.to(device)        # (1, L_full)\n",
    "    input_ids_prompt = tok_prompt.input_ids.to(device)    # (1, L_prompt)\n",
    "\n",
    "    L_full   = input_ids_full.size(1)\n",
    "    L_prompt = input_ids_prompt.size(1)\n",
    "\n",
    "\n",
    "    labels = input_ids_full.clone()\n",
    "    labels[:, :L_prompt] = -100\n",
    "\n",
    "    labels_padded = torch.full(\n",
    "        (1, L_full + 1),\n",
    "        -100,\n",
    "        dtype=torch.long,\n",
    "        device=device,\n",
    "    )\n",
    "    labels_padded[:, 1:] = labels\n",
    "\n",
    "    return input_ids_full, labels_padded, L_full\n",
    "\n",
    "NUM_EPOCHS = 15\n",
    "LR = 1e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(projector.parameters(), lr=LR)\n",
    "\n",
    "def evaluate_model():\n",
    "    projector.eval()\n",
    "    llama.eval()\n",
    "\n",
    "    all_f1, all_em = [], []\n",
    "\n",
    "    for sample in tqdm(test_ds, desc=\"Evaluating\", leave=False):\n",
    "        question = sample[\"question\"]\n",
    "        answer_gt = sample[\"answer\"]\n",
    "        img = sample[\"image\"]       # PIL Image\n",
    "\n",
    "        with torch.no_grad():\n",
    "            clip_inputs = clip_processor(\n",
    "                images=img,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "\n",
    "            vision_outputs = clip_vision(**clip_inputs)\n",
    "            # pooler_output: (B, vision_width)\n",
    "            vision_feat = vision_outputs.pooler_output    # (1, 768)\n",
    "\n",
    "            vision_embed_f32 = projector(vision_feat)     # (1, 2048) float32\n",
    "            vision_embed = vision_embed_f32.to(torch.float16)\n",
    "\n",
    "            prompt = f\"Question: {question}\\nAnswer:\"\n",
    "            tok = tokenizer(prompt, return_tensors=\"pt\")\n",
    "            input_ids = tok.input_ids.to(device)\n",
    "\n",
    "            text_emb = llama.model.embed_tokens(input_ids)  # (1, L, 2048) fp16\n",
    "\n",
    "            inputs_embeds = torch.cat(\n",
    "                [vision_embed.unsqueeze(1), text_emb],\n",
    "                dim=1\n",
    "            )\n",
    "\n",
    "            output = llama.generate(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                max_new_tokens=32,\n",
    "                do_sample=False,\n",
    "            )\n",
    "\n",
    "            pred = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            pred = pred.split(\"Answer:\")[-1].strip().split(\"\\n\")[0].strip()\n",
    "\n",
    "        all_f1.append(f1(pred, answer_gt))\n",
    "        all_em.append(em(pred, answer_gt))\n",
    "\n",
    "    mean_f1 = float(np.mean(all_f1))\n",
    "    mean_em = float(np.mean(all_em))\n",
    "    return mean_f1, mean_em\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    projector.train()\n",
    "    llama.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for sample in tqdm(train_ds, desc=f\"Epoch {epoch+1} / {NUM_EPOCHS}\"):\n",
    "        question = sample[\"question\"]\n",
    "        answer_gt = sample[\"answer\"]\n",
    "        img = sample[\"image\"]\n",
    "\n",
    "        clip_inputs = clip_processor(\n",
    "            images=img,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)  # pixel_values: (1,3,224,224)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            vision_outputs = clip_vision(**clip_inputs)\n",
    "            vision_feat = vision_outputs.pooler_output     # (1, 768) float32\n",
    "\n",
    "        vision_embed_f32 = projector(vision_feat)          # (1, 2048) float32\n",
    "        vision_embed = vision_embed_f32.to(torch.float16)  # (1, 2048) fp16\n",
    "\n",
    "        input_ids, labels_padded, L_full = build_inputs_and_labels(\n",
    "            question, answer_gt\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text_emb = llama.model.embed_tokens(input_ids)  # (1,L_full,2048), fp16\n",
    "\n",
    "        inputs_embeds = torch.cat(\n",
    "            [vision_embed.unsqueeze(1), text_emb],\n",
    "            dim=1\n",
    "        )  # (1, L_full+1, 2048)\n",
    "\n",
    "        outputs = llama(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            labels=labels_padded,\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_ds)\n",
    "    print(f\"\\n[Epoch {epoch+1}] train loss = {avg_loss:.4f}\")\n",
    "\n",
    "    val_f1, val_em = evaluate_model()\n",
    "    print(f\"[Epoch {epoch+1}] val F1 = {val_f1:.4f}, val EM = {val_em:.4f}\\n\")\n",
    "\n",
    "print(\"Training finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66987ad2-0ca1-44ba-ae6c-b6a42a86c4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
