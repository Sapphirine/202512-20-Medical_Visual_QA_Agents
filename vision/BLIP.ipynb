{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4045ef2-2d30-47e3-8bb8-235aa745db49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device=cuda  AMP=True  dtype=torch.float16\n",
      "[Len] max_pos=512, use max_q_len=32, max_a_len=8\n",
      "[Tokens] pad=0 bos=101 eos=102 vocab(len(tokenizer))=30522\n",
      "Train=19654  Val=6259  Test=6719\n",
      "[Preflight] Running 1 batch on CPU to catch index errors...\n",
      "[Preflight] OK on CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9479/2284293894.py:570: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=USE_FP16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ConstrainedDecode] answer_set=3225 trie_max_len=55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 1:   0%|          | 0/4914 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SANITY step=0] q_len=16 input_ids(min,max)=(0,27179) V=30522 max_pos=512\n",
      "[SANITY step=0] labels(min,max)=101,24471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9479/2284293894.py:630: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp, dtype=amp_dtype):\n",
      "train epoch 1:   0%|          | 1/4914 [00:00<59:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SANITY step=0] q_len=12 input_ids(min,max)=(0,16464) V=30522 max_pos=512\n",
      "[SANITY step=0] labels(min,max)=101,11467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 1:   0%|          | 3/4914 [00:01<27:44,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SANITY step=1] q_len=31 input_ids(min,max)=(0,25032) V=30522 max_pos=512\n",
      "[SANITY step=1] labels(min,max)=101,28873\n",
      "[SANITY step=1] q_len=22 input_ids(min,max)=(0,22520) V=30522 max_pos=512\n",
      "[SANITY step=1] labels(min,max)=101,21716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 1:   0%|          | 5/4914 [00:01<18:31,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SANITY step=2] q_len=32 input_ids(min,max)=(0,19466) V=30522 max_pos=512\n",
      "[SANITY step=2] labels(min,max)=101,20360\n",
      "[SANITY step=2] q_len=26 input_ids(min,max)=(0,26775) V=30522 max_pos=512\n",
      "[SANITY step=2] labels(min,max)=101,29181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 1:   0%|          | 7/4914 [00:01<15:25,  5.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SANITY step=3] q_len=12 input_ids(min,max)=(0,21101) V=30522 max_pos=512\n",
      "[SANITY step=3] labels(min,max)=101,22935\n",
      "[SANITY step=3] q_len=21 input_ids(min,max)=(0,19962) V=30522 max_pos=512\n",
      "[SANITY step=3] labels(min,max)=101,2748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 1:   0%|          | 9/4914 [00:02<14:13,  5.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SANITY step=4] q_len=10 input_ids(min,max)=(101,5648) V=30522 max_pos=512\n",
      "[SANITY step=4] labels(min,max)=101,16464\n",
      "[SANITY step=4] q_len=17 input_ids(min,max)=(0,27584) V=30522 max_pos=512\n",
      "[SANITY step=4] labels(min,max)=101,25147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 1:   7%|▋         | 325/4914 [00:50<11:27,  6.67it/s, loss=10.6587]/usr/local/lib/python3.12/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n",
      "eval:   0%|          | 0/1565 [00:00<?, ?it/s]/tmp/ipykernel_9479/2284293894.py:354: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp, dtype=amp_dtype):\n",
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] VAL acc=0.0000  yes/no=0.0000  open=0.0000  n=6259\n",
      "  -> saved BEST to ./runs/blip_pathvqa_best/best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] VAL acc=0.0000  yes/no=0.0000  open=0.0000  n=6259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] VAL acc=0.0000  yes/no=0.0000  open=0.0000  n=6259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] VAL acc=0.0000  yes/no=0.0000  open=0.0000  n=6259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] VAL acc=0.0000  yes/no=0.0000  open=0.0000  n=6259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] VAL acc=0.0000  yes/no=0.0000  open=0.0000  n=6259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] VAL acc=0.0000  yes/no=0.0000  open=0.0000  n=6259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] VAL acc=0.0000  yes/no=0.0000  open=0.0000  n=6259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 9:  20%|██        | 993/4914 [02:24<09:21,  6.98it/s, loss=0.0137]"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# ============ 环境稳态（必须放最前）============\n",
    "import os\n",
    "os.environ.setdefault(\"HF_HUB_ENABLE_HF_TRANSFER\", \"0\")  # 避免 hf_transfer 报错\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "# 如果你想强制同步定位 GPU 报错，打开下面这行（会慢很多）\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "import json, math, random, re, inspect\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForQuestionAnswering,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "# =========================================================\n",
    "# 0) CONFIG\n",
    "# =========================================================\n",
    "\n",
    "# —— 数据：HF PathVQA ——（推荐先跑通）\n",
    "USE_HF_DATASET = True\n",
    "HF_DATASET_NAME = \"flaviagiammarino/path-vqa\"\n",
    "HF_TRAIN_SPLIT = \"train\"\n",
    "HF_VAL_SPLIT = \"validation\"\n",
    "HF_TEST_SPLIT = \"test\"\n",
    "HF_IMAGE_COL = \"image\"\n",
    "HF_QUESTION_COL = \"question\"\n",
    "HF_ANSWER_COL = \"answer\"\n",
    "\n",
    "# —— 本地 JSONL（可选）——\n",
    "DATA_DIR = None\n",
    "IMAGE_ROOT = None\n",
    "TRAIN_JSONL = \"train.jsonl\"\n",
    "VAL_JSONL = \"val.jsonl\"\n",
    "TEST_JSONL = \"test.jsonl\"\n",
    "\n",
    "# 模型\n",
    "MODEL_NAME = \"Salesforce/blip-vqa-base\"\n",
    "\n",
    "# 输出\n",
    "OUTPUT_DIR = \"./runs/blip_pathvqa_best\"\n",
    "\n",
    "# 训练超参（先保守跑通）\n",
    "SEED = 42\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUM = 2\n",
    "LR = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.06\n",
    "\n",
    "MAX_QUESTION_LEN = 32   # 跑稳后可改 64\n",
    "MAX_ANSWER_LEN = 8      # 跑稳后可改 16\n",
    "USE_PROMPT = True\n",
    "\n",
    "NUM_WORKERS = 0         # notebook/共享环境最稳；跑通后你再加到 2/4\n",
    "PIN_MEMORY = True\n",
    "\n",
    "LOG_EVERY = 50\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "# 混合精度\n",
    "USE_FP16 = True\n",
    "USE_BF16 = False\n",
    "\n",
    "# Constrained decoding（只影响 eval）\n",
    "CONSTRAINED_DECODE = True\n",
    "ANSWER_VOCAB_SIZE = -1\n",
    "NUM_BEAMS = 3\n",
    "\n",
    "# Debug/预检\n",
    "CPU_PREFLIGHT_CHECK = True   # 先在 CPU 跑一个 batch，能把越界问题直接抓出来\n",
    "DEBUG_SANITY_STEPS = 5       # 训练前几个 step 打印/断言 token 范围\n",
    "\n",
    "# =========================================================\n",
    "# 1) utils\n",
    "# =========================================================\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def ensure_dir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def normalize_answer(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s).strip().lower()\n",
    "    s = s.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    s = s.strip(\" .,:;!?\\\"'()[]{}\")\n",
    "    if s in {\"y\", \"yeah\", \"yep\", \"true\"}:\n",
    "        s = \"yes\"\n",
    "    if s in {\"n\", \"nope\", \"nah\", \"false\"}:\n",
    "        s = \"no\"\n",
    "    return s\n",
    "\n",
    "# =========================================================\n",
    "# 2) dataset\n",
    "# =========================================================\n",
    "\n",
    "class HFVQADataset(Dataset):\n",
    "    \"\"\"\n",
    "    重要：缓存 answer 列，避免构造答案集合时把图片全读一遍。\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str, split: str, image_col: str, q_col: str, a_col: str, normalize: bool = True):\n",
    "        from datasets import load_dataset\n",
    "        self.ds = load_dataset(name, split=split)\n",
    "        self.image_col = image_col\n",
    "        self.q_col = q_col\n",
    "        self.a_col = a_col\n",
    "        self.normalize = normalize\n",
    "\n",
    "        for c in [image_col, q_col, a_col]:\n",
    "            if c not in self.ds.column_names:\n",
    "                raise ValueError(f\"Column '{c}' not in {self.ds.column_names}\")\n",
    "\n",
    "        raw_answers = list(self.ds[a_col])\n",
    "        self._answers = [normalize_answer(a) for a in raw_answers] if normalize else [str(a) for a in raw_answers]\n",
    "\n",
    "    def get_answers(self) -> List[str]:\n",
    "        return self._answers\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Image.Image, str, str]:\n",
    "        ex = self.ds[idx]\n",
    "        img = ex[self.image_col]\n",
    "        if not isinstance(img, Image.Image):\n",
    "            img = Image.fromarray(img).convert(\"RGB\")\n",
    "        q = str(ex[self.q_col])\n",
    "        a = self._answers[idx]\n",
    "        return img.convert(\"RGB\"), q, a\n",
    "\n",
    "\n",
    "class JsonlVQADataset(Dataset):\n",
    "    def __init__(self, jsonl_path: str, image_root: Optional[str], normalize: bool = True):\n",
    "        if not os.path.isfile(jsonl_path):\n",
    "            raise FileNotFoundError(f\"JSONL not found: {jsonl_path}\")\n",
    "        self.samples: List[Dict[str, Any]] = []\n",
    "        self.image_root = image_root\n",
    "        self.normalize = normalize\n",
    "        self._answers: List[str] = []\n",
    "\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                ex = json.loads(line)\n",
    "                for k in [\"image\", \"question\", \"answer\"]:\n",
    "                    if k not in ex:\n",
    "                        raise ValueError(f\"{jsonl_path} line {i} missing key '{k}', got keys={list(ex.keys())}\")\n",
    "                if self.normalize:\n",
    "                    ex[\"answer\"] = normalize_answer(ex[\"answer\"])\n",
    "                self.samples.append(ex)\n",
    "                self._answers.append(ex[\"answer\"])\n",
    "\n",
    "    def get_answers(self) -> List[str]:\n",
    "        return self._answers\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Image.Image, str, str]:\n",
    "        ex = self.samples[idx]\n",
    "        img_path = ex[\"image\"]\n",
    "        if self.image_root and not os.path.isabs(img_path):\n",
    "            img_path = os.path.join(self.image_root, img_path)\n",
    "        if not os.path.isfile(img_path):\n",
    "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        q = str(ex[\"question\"])\n",
    "        a = str(ex[\"answer\"])\n",
    "        return img, q, a\n",
    "\n",
    "# =========================================================\n",
    "# 3) constrained decoding trie\n",
    "# =========================================================\n",
    "\n",
    "class TrieNode:\n",
    "    __slots__ = (\"children\", \"is_end\")\n",
    "    def __init__(self):\n",
    "        self.children: Dict[int, \"TrieNode\"] = {}\n",
    "        self.is_end: bool = False\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, token_ids: List[int]) -> None:\n",
    "        node = self.root\n",
    "        for t in token_ids:\n",
    "            if t not in node.children:\n",
    "                node.children[t] = TrieNode()\n",
    "            node = node.children[t]\n",
    "        node.is_end = True\n",
    "\n",
    "    def next_tokens(self, prefix: List[int]) -> Tuple[List[int], bool]:\n",
    "        node = self.root\n",
    "        for t in prefix:\n",
    "            if t not in node.children:\n",
    "                return [], False\n",
    "            node = node.children[t]\n",
    "        return list(node.children.keys()), node.is_end\n",
    "\n",
    "def build_answer_set(ds: Dataset, max_answers: int) -> List[str]:\n",
    "    if hasattr(ds, \"get_answers\"):\n",
    "        answers = ds.get_answers()\n",
    "    else:\n",
    "        answers = [ds[i][2] for i in range(len(ds))]\n",
    "    cnt = Counter(answers)\n",
    "    if max_answers is not None and max_answers > 0:\n",
    "        ans_list = [a for a, _ in cnt.most_common(max_answers)]\n",
    "    else:\n",
    "        ans_list = list(cnt.keys())\n",
    "    for a in [\"yes\", \"no\"]:\n",
    "        if a not in ans_list:\n",
    "            ans_list.append(a)\n",
    "    return ans_list\n",
    "\n",
    "def build_trie_and_lookup(answer_list: List[str], tokenizer) -> Tuple[Trie, Dict[Tuple[int, ...], str], int]:\n",
    "    trie = Trie()\n",
    "    seq2ans: Dict[Tuple[int, ...], str] = {}\n",
    "    max_len = 0\n",
    "    for ans in answer_list:\n",
    "        ids = tokenizer(ans, add_special_tokens=False).input_ids\n",
    "        if not ids:\n",
    "            continue\n",
    "        trie.insert(ids)\n",
    "        seq2ans[tuple(ids)] = ans\n",
    "        max_len = max(max_len, len(ids))\n",
    "    return trie, seq2ans, max_len\n",
    "\n",
    "def make_prefix_allowed_tokens_fn(trie: Trie, bos_id: Optional[int], eos_id: int):\n",
    "    def _fn(batch_id: int, input_ids: torch.LongTensor) -> List[int]:\n",
    "        ids = input_ids.tolist()\n",
    "        if bos_id is not None and len(ids) > 0 and ids[0] == bos_id:\n",
    "            prefix = ids[1:]\n",
    "        else:\n",
    "            prefix = ids\n",
    "        nxt, is_end = trie.next_tokens(prefix)\n",
    "        allowed = list(nxt)\n",
    "        if is_end:\n",
    "            allowed.append(eos_id)\n",
    "        if not allowed:\n",
    "            allowed = [eos_id]\n",
    "        return allowed\n",
    "    return _fn\n",
    "\n",
    "# =========================================================\n",
    "# 4) collate（关键：图像/文本分开处理，确保 truncation 生效）\n",
    "# =========================================================\n",
    "\n",
    "@dataclass\n",
    "class Batch:\n",
    "    pixel_values: torch.FloatTensor\n",
    "    input_ids: torch.LongTensor\n",
    "    attention_mask: torch.LongTensor\n",
    "    labels: torch.LongTensor\n",
    "    answers: List[str]\n",
    "\n",
    "def make_collate_fn(processor: BlipProcessor, max_q_len: int, max_a_len: int):\n",
    "    tok = processor.tokenizer\n",
    "\n",
    "    def _collate(examples: List[Tuple[Image.Image, str, str]]) -> Batch:\n",
    "        images, questions, answers = zip(*examples)\n",
    "        if USE_PROMPT:\n",
    "            questions = tuple([f\"Question: {q} Answer:\" for q in questions])\n",
    "\n",
    "        # 图像\n",
    "        pixel = processor(images=list(images), return_tensors=\"pt\")\n",
    "\n",
    "        # 问题文本（强制 tokenizer，确保截断）\n",
    "        txt = tok(\n",
    "            list(questions),\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_q_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # 答案 labels\n",
    "        ans_tok = tok(\n",
    "            list(answers),\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_a_len,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "        labels = ans_tok.input_ids\n",
    "\n",
    "        # pad -> -100（loss ignore）\n",
    "        pad_id = tok.pad_token_id\n",
    "        labels = labels.clone()\n",
    "        labels[labels == pad_id] = -100\n",
    "\n",
    "        return Batch(\n",
    "            pixel_values=pixel[\"pixel_values\"],\n",
    "            input_ids=txt[\"input_ids\"],\n",
    "            attention_mask=txt[\"attention_mask\"],\n",
    "            labels=labels,\n",
    "            answers=list(answers),\n",
    "        )\n",
    "\n",
    "    return _collate\n",
    "\n",
    "# =========================================================\n",
    "# 5) eval\n",
    "# =========================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, processor, loader, device, use_amp, amp_dtype,\n",
    "             constrained, prefix_allowed_tokens_fn, seq2ans, max_gen_len, num_beams):\n",
    "    model.eval()\n",
    "    tok = processor.tokenizer\n",
    "    bos = tok.cls_token_id if tok.cls_token_id is not None else tok.bos_token_id\n",
    "    eos = tok.sep_token_id if tok.sep_token_id is not None else tok.eos_token_id\n",
    "    pad = tok.pad_token_id\n",
    "\n",
    "    total = correct = 0\n",
    "    total_yesno = correct_yesno = 0\n",
    "    total_open = correct_open = 0\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"eval\", leave=False):\n",
    "        pixel_values = batch.pixel_values.to(device, non_blocking=True)\n",
    "        input_ids = batch.input_ids.to(device, non_blocking=True)\n",
    "        attention_mask = batch.attention_mask.to(device, non_blocking=True)\n",
    "\n",
    "        gen_kwargs = dict(\n",
    "            max_length=1 + max_gen_len,\n",
    "            num_beams=num_beams,\n",
    "            do_sample=False,\n",
    "        )\n",
    "        if constrained:\n",
    "            gen_kwargs[\"prefix_allowed_tokens_fn\"] = prefix_allowed_tokens_fn\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp, dtype=amp_dtype):\n",
    "            gen_ids = model.generate(\n",
    "                pixel_values=pixel_values,\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                **gen_kwargs,\n",
    "            )\n",
    "\n",
    "        preds: List[str] = []\n",
    "        for seq in gen_ids:\n",
    "            ids = seq.tolist()\n",
    "            if bos is not None and len(ids) > 0 and ids[0] == bos:\n",
    "                ids = ids[1:]\n",
    "            if eos is not None and eos in ids:\n",
    "                ids = ids[:ids.index(eos)]\n",
    "            if pad is not None:\n",
    "                ids = [t for t in ids if t != pad]\n",
    "\n",
    "            if constrained and seq2ans is not None:\n",
    "                pred = seq2ans.get(tuple(ids), tok.decode(ids, skip_special_tokens=True))\n",
    "            else:\n",
    "                pred = tok.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "            preds.append(normalize_answer(pred))\n",
    "\n",
    "        gts = batch.answers\n",
    "        for p, gt in zip(preds, gts):\n",
    "            total += 1\n",
    "            ok = (p == gt)\n",
    "            correct += int(ok)\n",
    "\n",
    "            if gt in (\"yes\", \"no\"):\n",
    "                total_yesno += 1\n",
    "                correct_yesno += int(ok)\n",
    "            else:\n",
    "                total_open += 1\n",
    "                correct_open += int(ok)\n",
    "\n",
    "    return {\n",
    "        \"acc\": correct / max(1, total),\n",
    "        \"acc_yesno\": correct_yesno / max(1, total_yesno),\n",
    "        \"acc_open\": correct_open / max(1, total_open),\n",
    "        \"n\": total,\n",
    "        \"n_yesno\": total_yesno,\n",
    "        \"n_open\": total_open,\n",
    "    }\n",
    "\n",
    "# =========================================================\n",
    "# 6) main\n",
    "# =========================================================\n",
    "\n",
    "def main():\n",
    "    if USE_FP16 and USE_BF16:\n",
    "        raise ValueError(\"Choose only one: USE_FP16 or USE_BF16\")\n",
    "\n",
    "    set_seed(SEED)\n",
    "    ensure_dir(OUTPUT_DIR)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    use_amp = USE_FP16 or USE_BF16\n",
    "    amp_dtype = torch.float16 if USE_FP16 else (torch.bfloat16 if USE_BF16 else torch.float32)\n",
    "    print(f\"Device={device}  AMP={use_amp}  dtype={amp_dtype}\")\n",
    "\n",
    "    # 先在 CPU 加载，先做预检\n",
    "    processor = BlipProcessor.from_pretrained(MODEL_NAME)\n",
    "    model = BlipForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    tok = processor.tokenizer\n",
    "\n",
    "    # ---- 关键：保证 PAD 存在，否则 labels(-100) shift 可能出大坑\n",
    "    if tok.pad_token is None:\n",
    "        # 加一个 pad token 并扩展 embedding\n",
    "        tok.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "        model.resize_token_embeddings(len(tok))\n",
    "\n",
    "    pad_id = tok.pad_token_id\n",
    "    bos_id = tok.cls_token_id if tok.cls_token_id is not None else tok.bos_token_id\n",
    "    eos_id = tok.sep_token_id if tok.sep_token_id is not None else tok.eos_token_id\n",
    "    if bos_id is None:\n",
    "        raise ValueError(\"Cannot determine bos/cls token id for BLIP tokenizer.\")\n",
    "    if eos_id is None:\n",
    "        raise ValueError(\"Cannot determine eos/sep token id for BLIP tokenizer.\")\n",
    "\n",
    "    # 同步到 config（非常重要）\n",
    "    model.config.pad_token_id = pad_id\n",
    "    model.config.bos_token_id = bos_id\n",
    "    model.config.eos_token_id = eos_id\n",
    "    model.config.decoder_start_token_id = bos_id\n",
    "\n",
    "    # 有的版本把 text_config 分开存\n",
    "    if hasattr(model.config, \"text_config\") and model.config.text_config is not None:\n",
    "        model.config.text_config.pad_token_id = pad_id\n",
    "        model.config.text_config.bos_token_id = bos_id\n",
    "        model.config.text_config.eos_token_id = eos_id\n",
    "        model.config.text_config.decoder_start_token_id = bos_id\n",
    "\n",
    "    if hasattr(model, \"text_encoder\") and hasattr(model.text_encoder, \"config\"):\n",
    "        model.text_encoder.config.pad_token_id = pad_id\n",
    "    if hasattr(model, \"text_decoder\") and hasattr(model.text_decoder, \"config\"):\n",
    "        model.text_decoder.config.pad_token_id = pad_id\n",
    "        model.text_decoder.config.decoder_start_token_id = bos_id\n",
    "\n",
    "    # generation_config 也同步一下（避免 generate 内部出幺蛾子）\n",
    "    if getattr(model, \"generation_config\", None) is not None:\n",
    "        model.generation_config.pad_token_id = pad_id\n",
    "        model.generation_config.bos_token_id = bos_id\n",
    "        model.generation_config.eos_token_id = eos_id\n",
    "\n",
    "    # 长度上限（避免 position embedding 越界）\n",
    "    max_pos = int(model.text_encoder.config.max_position_embeddings)\n",
    "    max_q_len = min(MAX_QUESTION_LEN, max_pos)\n",
    "    max_a_len = min(MAX_ANSWER_LEN, max_pos)\n",
    "    print(f\"[Len] max_pos={max_pos}, use max_q_len={max_q_len}, max_a_len={max_a_len}\")\n",
    "    print(f\"[Tokens] pad={pad_id} bos={bos_id} eos={eos_id} vocab(len(tokenizer))={len(tok)}\")\n",
    "\n",
    "    # Load dataset\n",
    "    if USE_HF_DATASET:\n",
    "        train_ds = HFVQADataset(HF_DATASET_NAME, HF_TRAIN_SPLIT, HF_IMAGE_COL, HF_QUESTION_COL, HF_ANSWER_COL, normalize=True)\n",
    "        val_ds = HFVQADataset(HF_DATASET_NAME, HF_VAL_SPLIT, HF_IMAGE_COL, HF_QUESTION_COL, HF_ANSWER_COL, normalize=True)\n",
    "        test_ds = HFVQADataset(HF_DATASET_NAME, HF_TEST_SPLIT, HF_IMAGE_COL, HF_QUESTION_COL, HF_ANSWER_COL, normalize=True)\n",
    "    else:\n",
    "        if DATA_DIR is None or not os.path.isdir(DATA_DIR):\n",
    "            raise ValueError(\"Set USE_HF_DATASET=True or provide valid DATA_DIR for local JSONL.\")\n",
    "        image_root = IMAGE_ROOT if IMAGE_ROOT is not None else DATA_DIR\n",
    "        train_ds = JsonlVQADataset(os.path.join(DATA_DIR, TRAIN_JSONL), image_root=image_root, normalize=True)\n",
    "        val_ds = JsonlVQADataset(os.path.join(DATA_DIR, VAL_JSONL), image_root=image_root, normalize=True)\n",
    "        test_ds = JsonlVQADataset(os.path.join(DATA_DIR, TEST_JSONL), image_root=image_root, normalize=True)\n",
    "\n",
    "    print(f\"Train={len(train_ds)}  Val={len(val_ds)}  Test={len(test_ds)}\")\n",
    "\n",
    "    collate_fn = make_collate_fn(processor, max_q_len=max_q_len, max_a_len=max_a_len)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    # ---------- CPU 预检：抓 index 越界（强烈建议先开） ----------\n",
    "    if CPU_PREFLIGHT_CHECK:\n",
    "        print(\"[Preflight] Running 1 batch on CPU to catch index errors...\")\n",
    "        model.eval()\n",
    "        batch = next(iter(train_loader))\n",
    "        with torch.no_grad():\n",
    "            # CPU 上做一次范围检查\n",
    "            input_ids = batch.input_ids\n",
    "            labels = batch.labels\n",
    "\n",
    "            # 检查 input_ids 范围（注意：用 len(tok)，不是 tok.vocab_size）\n",
    "            V = len(tok)\n",
    "            assert int(input_ids.min()) >= 0\n",
    "            assert int(input_ids.max()) < V\n",
    "            assert input_ids.shape[1] <= max_pos\n",
    "\n",
    "            # 检查 labels（忽略 -100）\n",
    "            mask = labels != -100\n",
    "            if mask.any():\n",
    "                lb_min = int(labels[mask].min())\n",
    "                lb_max = int(labels[mask].max())\n",
    "                assert lb_min >= 0\n",
    "                assert lb_max < V\n",
    "\n",
    "            # 如果 forward 支持 decoder_input_ids，我们自己构造（绕开内部 shift 坑）\n",
    "            fwd_params = inspect.signature(model.forward).parameters\n",
    "            supports_decoder = \"decoder_input_ids\" in fwd_params\n",
    "\n",
    "            kwargs = dict(\n",
    "                pixel_values=batch.pixel_values,\n",
    "                input_ids=batch.input_ids,\n",
    "                attention_mask=batch.attention_mask,\n",
    "                labels=batch.labels,\n",
    "            )\n",
    "            if supports_decoder:\n",
    "                dec = batch.labels.clone()\n",
    "                dec[dec == -100] = pad_id\n",
    "                dec = torch.roll(dec, shifts=1, dims=1)\n",
    "                dec[:, 0] = bos_id\n",
    "                dec_attn = (dec != pad_id).long()\n",
    "                kwargs[\"decoder_input_ids\"] = dec\n",
    "                if \"decoder_attention_mask\" in fwd_params:\n",
    "                    kwargs[\"decoder_attention_mask\"] = dec_attn\n",
    "\n",
    "            _ = model(**kwargs)\n",
    "\n",
    "        print(\"[Preflight] OK on CPU.\")\n",
    "\n",
    "    # 移到 GPU\n",
    "    model.to(device)\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    # 约束解码（只用于 eval）\n",
    "    prefix_allowed_tokens_fn = None\n",
    "    seq2ans = None\n",
    "    trie_max_len = max_a_len\n",
    "    if CONSTRAINED_DECODE:\n",
    "        ans_list = build_answer_set(train_ds, ANSWER_VOCAB_SIZE)\n",
    "        trie, seq2ans, trie_max_len = build_trie_and_lookup(ans_list, tok)\n",
    "        prefix_allowed_tokens_fn = make_prefix_allowed_tokens_fn(trie, bos_id=bos_id, eos_id=eos_id)\n",
    "        print(f\"[ConstrainedDecode] answer_set={len(ans_list)} trie_max_len={trie_max_len}\")\n",
    "\n",
    "    # optimizer / scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    total_steps = math.ceil(len(train_loader) / max(1, GRAD_ACCUM)) * EPOCHS\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=USE_FP16)\n",
    "\n",
    "    best_val = -1.0\n",
    "    global_step = 0\n",
    "\n",
    "    # 保存 config\n",
    "    with open(os.path.join(OUTPUT_DIR, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        cfg = {k: v for k, v in globals().items() if k.isupper()}\n",
    "        json.dump(cfg, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # forward 是否支持 decoder_input_ids（动态兼容 transformers 版本）\n",
    "    fwd_params = inspect.signature(model.forward).parameters\n",
    "    supports_decoder = \"decoder_input_ids\" in fwd_params\n",
    "    supports_decoder_attn = \"decoder_attention_mask\" in fwd_params\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        running = 0.0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"train epoch {epoch}\", leave=False)\n",
    "        for step, batch in enumerate(pbar, start=1):\n",
    "            pixel_values = batch.pixel_values.to(device, non_blocking=True)\n",
    "            input_ids = batch.input_ids.to(device, non_blocking=True)\n",
    "            attention_mask = batch.attention_mask.to(device, non_blocking=True)\n",
    "            labels = batch.labels.to(device, non_blocking=True)\n",
    "\n",
    "            # ---- 训练前几个 step 做硬断言，定位越界 ----\n",
    "            if DEBUG_SANITY_STEPS > 0 and global_step < DEBUG_SANITY_STEPS:\n",
    "                V = len(tok)\n",
    "                print(f\"[SANITY step={global_step}] q_len={input_ids.shape[1]} \"\n",
    "                      f\"input_ids(min,max)=({int(input_ids.min())},{int(input_ids.max())}) V={V} max_pos={max_pos}\")\n",
    "                assert input_ids.shape[1] <= max_pos\n",
    "                assert int(input_ids.min()) >= 0\n",
    "                assert int(input_ids.max()) < V\n",
    "                mask = labels != -100\n",
    "                if mask.any():\n",
    "                    lb_min = int(labels[mask].min())\n",
    "                    lb_max = int(labels[mask].max())\n",
    "                    print(f\"[SANITY step={global_step}] labels(min,max)={lb_min},{lb_max}\")\n",
    "                    assert lb_min >= 0\n",
    "                    assert lb_max < V\n",
    "\n",
    "            kwargs = dict(\n",
    "                pixel_values=pixel_values,\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "\n",
    "            # ---- 如果支持 decoder_input_ids：自己构造，避免内部 shift -100 坑 ----\n",
    "            if supports_decoder:\n",
    "                dec = labels.clone()\n",
    "                dec[dec == -100] = pad_id\n",
    "                dec = torch.roll(dec, shifts=1, dims=1)\n",
    "                dec[:, 0] = bos_id\n",
    "                kwargs[\"decoder_input_ids\"] = dec\n",
    "                if supports_decoder_attn:\n",
    "                    kwargs[\"decoder_attention_mask\"] = (dec != pad_id).long()\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp, dtype=amp_dtype):\n",
    "                out = model(**kwargs)\n",
    "                loss = out.loss / max(1, GRAD_ACCUM)\n",
    "\n",
    "            if USE_FP16:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            running += loss.item() * max(1, GRAD_ACCUM)\n",
    "\n",
    "            if step % GRAD_ACCUM == 0:\n",
    "                if USE_FP16:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "                    optimizer.step()\n",
    "\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                global_step += 1\n",
    "\n",
    "                if global_step % LOG_EVERY == 0:\n",
    "                    pbar.set_postfix(loss=f\"{running / max(1, global_step):.4f}\")\n",
    "\n",
    "        # Validate\n",
    "        val_metrics = evaluate(\n",
    "            model, processor, val_loader, device, use_amp, amp_dtype,\n",
    "            CONSTRAINED_DECODE, prefix_allowed_tokens_fn, seq2ans,\n",
    "            max_gen_len=min(max_a_len, trie_max_len),\n",
    "            num_beams=NUM_BEAMS,\n",
    "        )\n",
    "        print(f\"[Epoch {epoch}] VAL acc={val_metrics['acc']:.4f}  yes/no={val_metrics['acc_yesno']:.4f}  open={val_metrics['acc_open']:.4f}  n={val_metrics['n']}\")\n",
    "\n",
    "        # Save best\n",
    "        if val_metrics[\"acc\"] > best_val:\n",
    "            best_val = val_metrics[\"acc\"]\n",
    "            best_dir = os.path.join(OUTPUT_DIR, \"best\")\n",
    "            ensure_dir(best_dir)\n",
    "            model.save_pretrained(best_dir)\n",
    "            processor.save_pretrained(best_dir)\n",
    "            with open(os.path.join(best_dir, \"val_metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(val_metrics, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"  -> saved BEST to {best_dir}\")\n",
    "\n",
    "    # Test best\n",
    "    print(\"\\nLoading BEST checkpoint for TEST...\")\n",
    "    best_dir = os.path.join(OUTPUT_DIR, \"best\")\n",
    "    model = BlipForQuestionAnswering.from_pretrained(best_dir).to(device)\n",
    "    processor = BlipProcessor.from_pretrained(best_dir)\n",
    "\n",
    "    test_metrics = evaluate(\n",
    "        model, processor, test_loader, device, use_amp, amp_dtype,\n",
    "        CONSTRAINED_DECODE, prefix_allowed_tokens_fn, seq2ans,\n",
    "        max_gen_len=min(max_a_len, trie_max_len),\n",
    "        num_beams=NUM_BEAMS,\n",
    "    )\n",
    "    print(f\"[BEST] TEST acc={test_metrics['acc']:.4f}  yes/no={test_metrics['acc_yesno']:.4f}  open={test_metrics['acc_open']:.4f}  n={test_metrics['n']}\")\n",
    "\n",
    "    with open(os.path.join(OUTPUT_DIR, \"test_metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(test_metrics, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b675b074-6e1c-4ab0-933b-fb6c765d5da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DEBUG HF_DATASET_NAME =\", HF_DATASET_NAME)\n",
    "print(\"DEBUG DATA_DIR =\", DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3358e28e-a739-402c-9a96-5afde246381e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device=cuda  AMP=True  dtype=torch.float16\n",
      "[Len] max_pos=512, use max_q_len=32, max_a_len=8, max_gen_len=8\n",
      "[Tokens] pad=0 bos=101 eos=102 vocab(len(tok))=30522\n",
      "Train=19654  Val=6259  Test=6719\n",
      "[Preflight] Running 1 batch on CPU...\n",
      "[Preflight] OK.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9479/1145000565.py:589: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=USE_FP16)\n",
      "train epoch 1:   0%|          | 0/4914 [00:00<?, ?it/s]/tmp/ipykernel_9479/1145000565.py:639: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp, dtype=amp_dtype):\n",
      "eval(gen):   0%|          | 0/1565 [00:00<?, ?it/s]/tmp/ipykernel_9479/1145000565.py:404: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp, dtype=amp_dtype):\n",
      "eval(gen):   0%|          | 2/1565 [00:00<03:19,  7.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Q : what have lost their nuclei?\n",
      "GT: neutrophils\n",
      "PR: cells cells of\n",
      "============================================================\n",
      "Q : whose nuclei have been lost?\n",
      "GT: neutrophils\n",
      "PR: one\n",
      "============================================================\n",
      "Q : are two small pulmonary arterioles packed with laminated swirls of fetal squamous cells?\n",
      "GT: yes\n",
      "PR: yes\n",
      "============================================================\n",
      "Q : what is acute viral hepatitis characterized by?\n",
      "GT: predominantly lymphocytic infiltrate\n",
      "PR: cells cells of\n",
      "============================================================\n",
      "Q : what do the cells have?\n",
      "GT: wavy nuclei\n",
      "PR: granmas\n",
      "============================================================\n",
      "Q : do the cells have wavy nuclei?\n",
      "GT: yes\n",
      "PR: yes\n",
      "============================================================\n",
      "Q : do individual myocardial fibres have wavy nuclei?\n",
      "GT: no\n",
      "PR: yes\n",
      "============================================================\n",
      "Q : where is this area in the body?\n",
      "GT: abdomen\n",
      "PR: abdomen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval(gen):   0%|          | 4/1565 [00:00<03:46,  6.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Q : what does this image show?\n",
      "GT: peritoneal carcinomatosis\n",
      "PR: close up close up view of\n",
      "============================================================\n",
      "Q : does this image show peritoneal carcinomatosis, metastatic tumor covering all of the abdominal viscera?\n",
      "GT: yes\n",
      "PR: yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] VAL exact=0.4459 f1=0.4667 yesno=0.8166 open_exact=0.0763 open_f1=0.1178 (n=6259, yesno=3125, open=3134)\n",
      "  -> saved BEST by exact_acc=0.4459 to ./runs/blip_pathvqa_best/best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] VAL exact=0.4907 f1=0.5133 yesno=0.8627 open_exact=0.1197 open_f1=0.1649 (n=6259, yesno=3125, open=3134)\n",
      "  -> saved BEST by exact_acc=0.4907 to ./runs/blip_pathvqa_best/best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] VAL exact=0.4918 f1=0.5168 yesno=0.8637 open_exact=0.1209 open_f1=0.1710 (n=6259, yesno=3125, open=3134)\n",
      "  -> saved BEST by exact_acc=0.4918 to ./runs/blip_pathvqa_best/best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] VAL exact=0.5001 f1=0.5264 yesno=0.8774 open_exact=0.1238 open_f1=0.1764 (n=6259, yesno=3125, open=3134)\n",
      "  -> saved BEST by exact_acc=0.5001 to ./runs/blip_pathvqa_best/best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] VAL exact=0.4967 f1=0.5250 yesno=0.8746 open_exact=0.1200 open_f1=0.1765 (n=6259, yesno=3125, open=3134)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] VAL exact=0.5113 f1=0.5431 yesno=0.8810 open_exact=0.1426 open_f1=0.2061 (n=6259, yesno=3125, open=3134)\n",
      "  -> saved BEST by exact_acc=0.5113 to ./runs/blip_pathvqa_best/best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 7:   1%|▏         | 68/4914 [00:09<11:17,  7.15it/s, loss=0.0003]"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# ============ 环境稳态（建议放最前）============\n",
    "import os\n",
    "os.environ.setdefault(\"HF_HUB_ENABLE_HF_TRANSFER\", \"0\")\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "# 如需强制同步定位 CUDA 报错：取消注释（会明显变慢）\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "import json, math, random, re, inspect, warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForQuestionAnswering,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "# 允许加载截断图（PathVQA/某些 HF 数据里偶尔会有）\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "warnings.filterwarnings(\"ignore\", message=\"Truncated File Read\")\n",
    "\n",
    "# =========================================================\n",
    "# 0) CONFIG（只改这里）\n",
    "# =========================================================\n",
    "\n",
    "# 数据：HuggingFace PathVQA\n",
    "USE_HF_DATASET = True\n",
    "HF_DATASET_NAME = \"flaviagiammarino/path-vqa\"\n",
    "HF_TRAIN_SPLIT = \"train\"\n",
    "HF_VAL_SPLIT = \"validation\"\n",
    "HF_TEST_SPLIT = \"test\"\n",
    "HF_IMAGE_COL = \"image\"\n",
    "HF_QUESTION_COL = \"question\"\n",
    "HF_ANSWER_COL = \"answer\"\n",
    "\n",
    "# 若用本地 JSONL（可选）\n",
    "DATA_DIR = None\n",
    "IMAGE_ROOT = None\n",
    "TRAIN_JSONL = \"train.jsonl\"\n",
    "VAL_JSONL = \"val.jsonl\"\n",
    "TEST_JSONL = \"test.jsonl\"\n",
    "\n",
    "# 模型\n",
    "MODEL_NAME = \"Salesforce/blip-vqa-base\"\n",
    "\n",
    "# 输出\n",
    "OUTPUT_DIR = \"./runs/blip_pathvqa_best\"\n",
    "BEST_SUBDIR = \"best\"\n",
    "\n",
    "# 训练超参\n",
    "SEED = 42\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUM = 2\n",
    "LR = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.06\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "# 文本长度\n",
    "MAX_QUESTION_LEN = 32\n",
    "MAX_ANSWER_LEN = 8   # teacher forcing 标签长度（训练）\n",
    "MAX_GEN_LEN = 8      # 推理生成长度（评估）；建议与 MAX_ANSWER_LEN 保持一致\n",
    "\n",
    "USE_PROMPT = True\n",
    "\n",
    "# DataLoader\n",
    "NUM_WORKERS = 0         # 共享/Notebook 环境最稳；跑稳后可改 2/4\n",
    "PIN_MEMORY = True\n",
    "\n",
    "# 混合精度\n",
    "USE_FP16 = True\n",
    "USE_BF16 = False\n",
    "\n",
    "# 评估生成参数（对齐你的独立评测脚本）\n",
    "NUM_BEAMS = 3\n",
    "\n",
    "# 是否在训练中打印一些 val 样例（对齐独立评测的打印风格）\n",
    "EVAL_PRINT_SAMPLES = 10\n",
    "\n",
    "# 选择保存 best 的指标（对齐“准确率第一”的诉求）\n",
    "# 可选: \"exact_acc\", \"token_f1\", \"open_exact_acc\", \"yesno_acc\"\n",
    "BEST_METRIC = \"exact_acc\"\n",
    "\n",
    "# 评估是否启用 constrained decoding（默认关：完全对齐你独立评测）\n",
    "EVAL_CONSTRAINED_DECODE = False\n",
    "ANSWER_VOCAB_SIZE = -1   # constrained decode 用；-1=所有训练答案\n",
    "\n",
    "# Debug/预检\n",
    "CPU_PREFLIGHT_CHECK = True\n",
    "DEBUG_SANITY_STEPS = 3   # 前几个 step 做 token 范围断言\n",
    "\n",
    "# =========================================================\n",
    "# 1) 评估指标（完全对齐你独立评测脚本）\n",
    "# =========================================================\n",
    "\n",
    "def normalize_answer(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\b(the|a|an)\\b\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def token_f1(pred: str, gt: str) -> float:\n",
    "    p = pred.split()\n",
    "    g = gt.split()\n",
    "    if len(p) == 0 or len(g) == 0:\n",
    "        return 0.0\n",
    "    common = set(p) & set(g)\n",
    "    if len(common) == 0:\n",
    "        return 0.0\n",
    "    prec = len(common) / len(p)\n",
    "    rec = len(common) / len(g)\n",
    "    return 2 * prec * rec / (prec + rec)\n",
    "\n",
    "# =========================================================\n",
    "# 2) utils\n",
    "# =========================================================\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def ensure_dir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# =========================================================\n",
    "# 3) dataset\n",
    "# =========================================================\n",
    "\n",
    "class HFVQADataset(Dataset):\n",
    "    \"\"\"\n",
    "    注意：返回 (image, question, answer_raw)\n",
    "    answer 不做 normalize（训练要用原始答案 tokenization）。\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str, split: str, image_col: str, q_col: str, a_col: str):\n",
    "        from datasets import load_dataset\n",
    "        self.ds = load_dataset(name, split=split)\n",
    "        self.image_col = image_col\n",
    "        self.q_col = q_col\n",
    "        self.a_col = a_col\n",
    "\n",
    "        for c in [image_col, q_col, a_col]:\n",
    "            if c not in self.ds.column_names:\n",
    "                raise ValueError(f\"Column '{c}' not in {self.ds.column_names}\")\n",
    "\n",
    "        # 缓存 raw answers，构 trie 时不会把图片全加载一遍\n",
    "        self._answers_raw = [str(x) for x in list(self.ds[a_col])]\n",
    "\n",
    "    def get_answers_raw(self) -> List[str]:\n",
    "        return self._answers_raw\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Image.Image, str, str]:\n",
    "        ex = self.ds[idx]\n",
    "        img = ex[self.image_col]\n",
    "        if not isinstance(img, Image.Image):\n",
    "            img = Image.fromarray(img).convert(\"RGB\")\n",
    "        q = str(ex[self.q_col])\n",
    "        a = self._answers_raw[idx]\n",
    "        return img.convert(\"RGB\"), q, a\n",
    "\n",
    "\n",
    "class JsonlVQADataset(Dataset):\n",
    "    def __init__(self, jsonl_path: str, image_root: Optional[str]):\n",
    "        if not os.path.isfile(jsonl_path):\n",
    "            raise FileNotFoundError(f\"JSONL not found: {jsonl_path}\")\n",
    "        self.samples: List[Dict[str, Any]] = []\n",
    "        self.image_root = image_root\n",
    "        self._answers_raw: List[str] = []\n",
    "\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                ex = json.loads(line)\n",
    "                for k in [\"image\", \"question\", \"answer\"]:\n",
    "                    if k not in ex:\n",
    "                        raise ValueError(f\"{jsonl_path} line {i} missing key '{k}', got keys={list(ex.keys())}\")\n",
    "                self.samples.append(ex)\n",
    "                self._answers_raw.append(str(ex[\"answer\"]))\n",
    "\n",
    "    def get_answers_raw(self) -> List[str]:\n",
    "        return self._answers_raw\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Image.Image, str, str]:\n",
    "        ex = self.samples[idx]\n",
    "        img_path = ex[\"image\"]\n",
    "        if self.image_root and not os.path.isabs(img_path):\n",
    "            img_path = os.path.join(self.image_root, img_path)\n",
    "        if not os.path.isfile(img_path):\n",
    "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        q = str(ex[\"question\"])\n",
    "        a = str(ex[\"answer\"])\n",
    "        return img, q, a\n",
    "\n",
    "# =========================================================\n",
    "# 4) constrained decoding（可选）\n",
    "# =========================================================\n",
    "\n",
    "class TrieNode:\n",
    "    __slots__ = (\"children\", \"is_end\")\n",
    "    def __init__(self):\n",
    "        self.children: Dict[int, \"TrieNode\"] = {}\n",
    "        self.is_end: bool = False\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, token_ids: List[int]) -> None:\n",
    "        node = self.root\n",
    "        for t in token_ids:\n",
    "            if t not in node.children:\n",
    "                node.children[t] = TrieNode()\n",
    "            node = node.children[t]\n",
    "        node.is_end = True\n",
    "\n",
    "    def next_tokens(self, prefix: List[int]) -> Tuple[List[int], bool]:\n",
    "        node = self.root\n",
    "        for t in prefix:\n",
    "            if t not in node.children:\n",
    "                return [], False\n",
    "            node = node.children[t]\n",
    "        return list(node.children.keys()), node.is_end\n",
    "\n",
    "def build_answer_set_raw(ds: Dataset, max_answers: int) -> List[str]:\n",
    "    if hasattr(ds, \"get_answers_raw\"):\n",
    "        answers = ds.get_answers_raw()\n",
    "    else:\n",
    "        answers = [ds[i][2] for i in range(len(ds))]\n",
    "    cnt = Counter(answers)\n",
    "    if max_answers is not None and max_answers > 0:\n",
    "        ans_list = [a for a, _ in cnt.most_common(max_answers)]\n",
    "    else:\n",
    "        ans_list = list(cnt.keys())\n",
    "    # 强制包含 yes/no（raw 里可能大小写不一致，但无所谓）\n",
    "    for a in [\"yes\", \"no\", \"Yes\", \"No\"]:\n",
    "        if a not in ans_list:\n",
    "            ans_list.append(a)\n",
    "    return ans_list\n",
    "\n",
    "def build_trie(answer_list: List[str], tokenizer) -> Trie:\n",
    "    trie = Trie()\n",
    "    for ans in answer_list:\n",
    "        ids = tokenizer(ans, add_special_tokens=False).input_ids\n",
    "        if ids:\n",
    "            trie.insert(ids)\n",
    "    return trie\n",
    "\n",
    "def make_prefix_allowed_tokens_fn(trie: Trie, bos_id: Optional[int], eos_id: int):\n",
    "    def _fn(batch_id: int, input_ids: torch.LongTensor) -> List[int]:\n",
    "        ids = input_ids.tolist()\n",
    "        if bos_id is not None and len(ids) > 0 and ids[0] == bos_id:\n",
    "            prefix = ids[1:]\n",
    "        else:\n",
    "            prefix = ids\n",
    "        nxt, is_end = trie.next_tokens(prefix)\n",
    "        allowed = list(nxt)\n",
    "        if is_end:\n",
    "            allowed.append(eos_id)\n",
    "        if not allowed:\n",
    "            allowed = [eos_id]\n",
    "        return allowed\n",
    "    return _fn\n",
    "\n",
    "# =========================================================\n",
    "# 5) collate（图像/文本分开处理，确保 truncation 生效）\n",
    "# =========================================================\n",
    "\n",
    "@dataclass\n",
    "class Batch:\n",
    "    pixel_values: torch.FloatTensor\n",
    "    input_ids: torch.LongTensor\n",
    "    attention_mask: torch.LongTensor\n",
    "    labels: torch.LongTensor\n",
    "    answers_raw: List[str]\n",
    "    questions_raw: List[str]\n",
    "\n",
    "def make_collate_fn(processor: BlipProcessor, max_q_len: int, max_a_len: int):\n",
    "    tok = processor.tokenizer\n",
    "\n",
    "    def _collate(examples: List[Tuple[Image.Image, str, str]]) -> Batch:\n",
    "        images, questions, answers = zip(*examples)\n",
    "\n",
    "        questions_in = list(questions)\n",
    "        if USE_PROMPT:\n",
    "            questions_in = [f\"Question: {q} Answer:\" for q in questions_in]\n",
    "\n",
    "        # 图像\n",
    "        pixel = processor(images=list(images), return_tensors=\"pt\")\n",
    "\n",
    "        # 问题（强制 tokenizer 截断）\n",
    "        txt = tok(\n",
    "            questions_in,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_q_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # 答案 labels（teacher forcing）\n",
    "        ans_tok = tok(\n",
    "            list(answers),\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_a_len,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "        labels = ans_tok.input_ids.clone()\n",
    "        labels[labels == tok.pad_token_id] = -100\n",
    "\n",
    "        return Batch(\n",
    "            pixel_values=pixel[\"pixel_values\"],\n",
    "            input_ids=txt[\"input_ids\"],\n",
    "            attention_mask=txt[\"attention_mask\"],\n",
    "            labels=labels,\n",
    "            answers_raw=list(answers),\n",
    "            questions_raw=list(questions),\n",
    "        )\n",
    "\n",
    "    return _collate\n",
    "\n",
    "# =========================================================\n",
    "# 6) decode + evaluate（对齐独立评测）\n",
    "# =========================================================\n",
    "\n",
    "def decode_one(gen_ids_1d: List[int], tokenizer, bos_id, eos_id, pad_id) -> str:\n",
    "    ids = list(gen_ids_1d)\n",
    "    if bos_id is not None and len(ids) > 0 and ids[0] == bos_id:\n",
    "        ids = ids[1:]\n",
    "    if eos_id is not None and eos_id in ids:\n",
    "        ids = ids[:ids.index(eos_id)]\n",
    "    if pad_id is not None:\n",
    "        ids = [t for t in ids if t != pad_id]\n",
    "    return tokenizer.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_generation(\n",
    "    model,\n",
    "    processor,\n",
    "    loader,\n",
    "    device,\n",
    "    use_amp,\n",
    "    amp_dtype,\n",
    "    max_gen_len: int,\n",
    "    num_beams: int,\n",
    "    prefix_allowed_tokens_fn=None,\n",
    "    print_samples: int = 0,\n",
    "):\n",
    "    tok = processor.tokenizer\n",
    "    pad_id = tok.pad_token_id\n",
    "    bos_id = tok.cls_token_id if tok.cls_token_id is not None else tok.bos_token_id\n",
    "    eos_id = tok.sep_token_id if tok.sep_token_id is not None else tok.eos_token_id\n",
    "\n",
    "    total = 0\n",
    "    exact = 0\n",
    "    f1_sum = 0.0\n",
    "\n",
    "    yesno_total = 0\n",
    "    yesno_correct = 0\n",
    "\n",
    "    open_total = 0\n",
    "    open_exact = 0\n",
    "    open_f1_sum = 0.0\n",
    "\n",
    "    printed = 0\n",
    "\n",
    "    model.eval()\n",
    "    for batch in tqdm(loader, desc=\"eval(gen)\", leave=False):\n",
    "        pixel_values = batch.pixel_values.to(device, non_blocking=True)\n",
    "        input_ids = batch.input_ids.to(device, non_blocking=True)\n",
    "        attention_mask = batch.attention_mask.to(device, non_blocking=True)\n",
    "\n",
    "        gen_kwargs = dict(\n",
    "            max_length=1 + max_gen_len,\n",
    "            num_beams=num_beams,\n",
    "            do_sample=False,\n",
    "        )\n",
    "        if prefix_allowed_tokens_fn is not None:\n",
    "            gen_kwargs[\"prefix_allowed_tokens_fn\"] = prefix_allowed_tokens_fn\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp, dtype=amp_dtype):\n",
    "            gen_out = model.generate(\n",
    "                pixel_values=pixel_values,\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                **gen_kwargs,\n",
    "            )\n",
    "\n",
    "        # per-sample\n",
    "        for seq, gt_raw, q_raw in zip(gen_out, batch.answers_raw, batch.questions_raw):\n",
    "            pred_raw = decode_one(seq.tolist(), tok, bos_id, eos_id, pad_id)\n",
    "\n",
    "            pred = normalize_answer(pred_raw)\n",
    "            gt = normalize_answer(gt_raw)\n",
    "\n",
    "            total += 1\n",
    "            if pred == gt:\n",
    "                exact += 1\n",
    "            f1_sum += token_f1(pred, gt)\n",
    "\n",
    "            if gt in [\"yes\", \"no\"]:\n",
    "                yesno_total += 1\n",
    "                if pred == gt:\n",
    "                    yesno_correct += 1\n",
    "            else:\n",
    "                open_total += 1\n",
    "                if pred == gt:\n",
    "                    open_exact += 1\n",
    "                open_f1_sum += token_f1(pred, gt)\n",
    "\n",
    "            if printed < print_samples:\n",
    "                print(\"=\" * 60)\n",
    "                print(\"Q :\", q_raw)\n",
    "                print(\"GT:\", gt)\n",
    "                print(\"PR:\", pred)\n",
    "                printed += 1\n",
    "\n",
    "    metrics = {\n",
    "        \"exact_acc\": exact / max(1, total),\n",
    "        \"token_f1\": f1_sum / max(1, total),\n",
    "        \"yesno_acc\": yesno_correct / max(1, yesno_total),\n",
    "        \"open_exact_acc\": open_exact / max(1, open_total),\n",
    "        \"open_token_f1\": open_f1_sum / max(1, open_total),\n",
    "        \"n\": total,\n",
    "        \"n_yesno\": yesno_total,\n",
    "        \"n_open\": open_total,\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# =========================================================\n",
    "# 7) main\n",
    "# =========================================================\n",
    "\n",
    "def main():\n",
    "    if USE_FP16 and USE_BF16:\n",
    "        raise ValueError(\"Choose only one: USE_FP16 or USE_BF16\")\n",
    "\n",
    "    set_seed(SEED)\n",
    "    ensure_dir(OUTPUT_DIR)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    use_amp = USE_FP16 or USE_BF16\n",
    "    amp_dtype = torch.float16 if USE_FP16 else (torch.bfloat16 if USE_BF16 else torch.float32)\n",
    "    print(f\"Device={device}  AMP={use_amp}  dtype={amp_dtype}\")\n",
    "\n",
    "    # 先 CPU 加载，做 pad/config 修复 + preflight\n",
    "    processor = BlipProcessor.from_pretrained(MODEL_NAME)\n",
    "    model = BlipForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
    "    tok = processor.tokenizer\n",
    "\n",
    "    # ---- 确保 pad_token 存在（避免 labels shift 的坑）\n",
    "    if tok.pad_token is None:\n",
    "        tok.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "        model.resize_token_embeddings(len(tok))\n",
    "\n",
    "    pad_id = tok.pad_token_id\n",
    "    bos_id = tok.cls_token_id if tok.cls_token_id is not None else tok.bos_token_id\n",
    "    eos_id = tok.sep_token_id if tok.sep_token_id is not None else tok.eos_token_id\n",
    "    if bos_id is None or eos_id is None:\n",
    "        raise ValueError(\"Cannot determine BOS/EOS token id for BLIP tokenizer.\")\n",
    "\n",
    "    # 同步 config（很关键）\n",
    "    model.config.pad_token_id = pad_id\n",
    "    model.config.bos_token_id = bos_id\n",
    "    model.config.eos_token_id = eos_id\n",
    "    model.config.decoder_start_token_id = bos_id\n",
    "    if getattr(model, \"generation_config\", None) is not None:\n",
    "        model.generation_config.pad_token_id = pad_id\n",
    "        model.generation_config.bos_token_id = bos_id\n",
    "        model.generation_config.eos_token_id = eos_id\n",
    "\n",
    "    # 长度上限（避免 position embedding 越界）\n",
    "    max_pos = int(model.text_encoder.config.max_position_embeddings)\n",
    "    max_q_len = min(MAX_QUESTION_LEN, max_pos)\n",
    "    max_a_len = min(MAX_ANSWER_LEN, max_pos)\n",
    "    print(f\"[Len] max_pos={max_pos}, use max_q_len={max_q_len}, max_a_len={max_a_len}, max_gen_len={MAX_GEN_LEN}\")\n",
    "    print(f\"[Tokens] pad={pad_id} bos={bos_id} eos={eos_id} vocab(len(tok))={len(tok)}\")\n",
    "\n",
    "    # Load dataset\n",
    "    if USE_HF_DATASET:\n",
    "        train_ds = HFVQADataset(HF_DATASET_NAME, HF_TRAIN_SPLIT, HF_IMAGE_COL, HF_QUESTION_COL, HF_ANSWER_COL)\n",
    "        val_ds = HFVQADataset(HF_DATASET_NAME, HF_VAL_SPLIT, HF_IMAGE_COL, HF_QUESTION_COL, HF_ANSWER_COL)\n",
    "        test_ds = HFVQADataset(HF_DATASET_NAME, HF_TEST_SPLIT, HF_IMAGE_COL, HF_QUESTION_COL, HF_ANSWER_COL)\n",
    "    else:\n",
    "        if DATA_DIR is None or not os.path.isdir(DATA_DIR):\n",
    "            raise ValueError(\"Set USE_HF_DATASET=True or provide valid DATA_DIR for local JSONL.\")\n",
    "        image_root = IMAGE_ROOT if IMAGE_ROOT is not None else DATA_DIR\n",
    "        train_ds = JsonlVQADataset(os.path.join(DATA_DIR, TRAIN_JSONL), image_root=image_root)\n",
    "        val_ds = JsonlVQADataset(os.path.join(DATA_DIR, VAL_JSONL), image_root=image_root)\n",
    "        test_ds = JsonlVQADataset(os.path.join(DATA_DIR, TEST_JSONL), image_root=image_root)\n",
    "\n",
    "    print(f\"Train={len(train_ds)}  Val={len(val_ds)}  Test={len(test_ds)}\")\n",
    "\n",
    "    collate_fn = make_collate_fn(processor, max_q_len=max_q_len, max_a_len=max_a_len)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    # ---------- CPU 预检：抓 index 越界 ----------\n",
    "    if CPU_PREFLIGHT_CHECK:\n",
    "        print(\"[Preflight] Running 1 batch on CPU...\")\n",
    "        model.eval()\n",
    "        batch = next(iter(train_loader))\n",
    "        with torch.no_grad():\n",
    "            V = len(tok)\n",
    "            assert int(batch.input_ids.min()) >= 0\n",
    "            assert int(batch.input_ids.max()) < V\n",
    "            mask = batch.labels != -100\n",
    "            if mask.any():\n",
    "                assert int(batch.labels[mask].min()) >= 0\n",
    "                assert int(batch.labels[mask].max()) < V\n",
    "\n",
    "            fwd_params = inspect.signature(model.forward).parameters\n",
    "            supports_decoder = \"decoder_input_ids\" in fwd_params\n",
    "\n",
    "            kwargs = dict(\n",
    "                pixel_values=batch.pixel_values,\n",
    "                input_ids=batch.input_ids,\n",
    "                attention_mask=batch.attention_mask,\n",
    "                labels=batch.labels,\n",
    "            )\n",
    "            if supports_decoder:\n",
    "                dec = batch.labels.clone()\n",
    "                dec[dec == -100] = pad_id\n",
    "                dec = torch.roll(dec, shifts=1, dims=1)\n",
    "                dec[:, 0] = bos_id\n",
    "                kwargs[\"decoder_input_ids\"] = dec\n",
    "                if \"decoder_attention_mask\" in fwd_params:\n",
    "                    kwargs[\"decoder_attention_mask\"] = (dec != pad_id).long()\n",
    "\n",
    "            _ = model(**kwargs)\n",
    "\n",
    "        print(\"[Preflight] OK.\")\n",
    "\n",
    "    # 评估：可选 constrained decoding（与独立评测默认保持一致=关）\n",
    "    prefix_allowed_tokens_fn = None\n",
    "    if EVAL_CONSTRAINED_DECODE:\n",
    "        ans_list = build_answer_set_raw(train_ds, ANSWER_VOCAB_SIZE)\n",
    "        trie = build_trie(ans_list, tok)\n",
    "        prefix_allowed_tokens_fn = make_prefix_allowed_tokens_fn(trie, bos_id=bos_id, eos_id=eos_id)\n",
    "        print(f\"[Eval ConstrainedDecode] answer_set={len(ans_list)}\")\n",
    "\n",
    "    # Move to GPU\n",
    "    model.to(device)\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    # optimizer / scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    total_steps = math.ceil(len(train_loader) / max(1, GRAD_ACCUM)) * EPOCHS\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=USE_FP16)\n",
    "\n",
    "    # forward 是否支持 decoder_input_ids\n",
    "    fwd_params = inspect.signature(model.forward).parameters\n",
    "    supports_decoder = \"decoder_input_ids\" in fwd_params\n",
    "    supports_decoder_attn = \"decoder_attention_mask\" in fwd_params\n",
    "\n",
    "    # 保存 config\n",
    "    with open(os.path.join(OUTPUT_DIR, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        cfg = {k: v for k, v in globals().items() if k.isupper()}\n",
    "        json.dump(cfg, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    best_score = -1e9\n",
    "    best_metrics = None\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        running_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"train epoch {epoch}\", leave=False)\n",
    "        for step, batch in enumerate(pbar, start=1):\n",
    "            pixel_values = batch.pixel_values.to(device, non_blocking=True)\n",
    "            input_ids = batch.input_ids.to(device, non_blocking=True)\n",
    "            attention_mask = batch.attention_mask.to(device, non_blocking=True)\n",
    "            labels = batch.labels.to(device, non_blocking=True)\n",
    "\n",
    "            # sanity（只检查前几个 global step）\n",
    "            if DEBUG_SANITY_STEPS > 0 and global_step < DEBUG_SANITY_STEPS:\n",
    "                V = len(tok)\n",
    "                assert int(input_ids.min()) >= 0 and int(input_ids.max()) < V\n",
    "\n",
    "            kwargs = dict(\n",
    "                pixel_values=pixel_values,\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "\n",
    "            # 自己构造 decoder_input_ids（规避内部 shift 的坑）\n",
    "            if supports_decoder:\n",
    "                dec = labels.clone()\n",
    "                dec[dec == -100] = pad_id\n",
    "                dec = torch.roll(dec, shifts=1, dims=1)\n",
    "                dec[:, 0] = bos_id\n",
    "                kwargs[\"decoder_input_ids\"] = dec\n",
    "                if supports_decoder_attn:\n",
    "                    kwargs[\"decoder_attention_mask\"] = (dec != pad_id).long()\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp, dtype=amp_dtype):\n",
    "                out = model(**kwargs)\n",
    "                loss = out.loss / max(1, GRAD_ACCUM)\n",
    "\n",
    "            if USE_FP16:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            running_loss += loss.item() * max(1, GRAD_ACCUM)\n",
    "\n",
    "            if step % GRAD_ACCUM == 0:\n",
    "                if USE_FP16:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "                    optimizer.step()\n",
    "\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                global_step += 1\n",
    "\n",
    "                if global_step % 50 == 0:\n",
    "                    pbar.set_postfix(loss=f\"{running_loss / max(1, global_step):.4f}\")\n",
    "\n",
    "        # ====== VAL EVAL（对齐独立评测逻辑）======\n",
    "        val_metrics = evaluate_generation(\n",
    "            model, processor, val_loader, device,\n",
    "            use_amp=use_amp, amp_dtype=amp_dtype,\n",
    "            max_gen_len=MAX_GEN_LEN, num_beams=NUM_BEAMS,\n",
    "            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    "            print_samples=EVAL_PRINT_SAMPLES if epoch == 1 else 0,\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"[Epoch {epoch}] VAL \"\n",
    "            f\"exact={val_metrics['exact_acc']:.4f} \"\n",
    "            f\"f1={val_metrics['token_f1']:.4f} \"\n",
    "            f\"yesno={val_metrics['yesno_acc']:.4f} \"\n",
    "            f\"open_exact={val_metrics['open_exact_acc']:.4f} \"\n",
    "            f\"open_f1={val_metrics['open_token_f1']:.4f} \"\n",
    "            f\"(n={val_metrics['n']}, yesno={val_metrics['n_yesno']}, open={val_metrics['n_open']})\"\n",
    "        )\n",
    "\n",
    "        # 保存 best\n",
    "        score = float(val_metrics.get(BEST_METRIC, -1e9))\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_metrics = val_metrics\n",
    "\n",
    "            best_dir = os.path.join(OUTPUT_DIR, BEST_SUBDIR)\n",
    "            ensure_dir(best_dir)\n",
    "            model.save_pretrained(best_dir)\n",
    "            processor.save_pretrained(best_dir)\n",
    "\n",
    "            with open(os.path.join(best_dir, \"val_metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(val_metrics, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            print(f\"  -> saved BEST by {BEST_METRIC}={best_score:.4f} to {best_dir}\")\n",
    "\n",
    "    # ====== TEST BEST ======\n",
    "    print(\"\\nLoading BEST checkpoint for TEST...\")\n",
    "    best_dir = os.path.join(OUTPUT_DIR, BEST_SUBDIR)\n",
    "    model = BlipForQuestionAnswering.from_pretrained(best_dir).to(device)\n",
    "    processor = BlipProcessor.from_pretrained(best_dir)\n",
    "\n",
    "    test_metrics = evaluate_generation(\n",
    "        model, processor, test_loader, device,\n",
    "        use_amp=use_amp, amp_dtype=amp_dtype,\n",
    "        max_gen_len=MAX_GEN_LEN, num_beams=NUM_BEAMS,\n",
    "        prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    "        print_samples=EVAL_PRINT_SAMPLES,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"[BEST] TEST \"\n",
    "        f\"exact={test_metrics['exact_acc']:.4f} \"\n",
    "        f\"f1={test_metrics['token_f1']:.4f} \"\n",
    "        f\"yesno={test_metrics['yesno_acc']:.4f} \"\n",
    "        f\"open_exact={test_metrics['open_exact_acc']:.4f} \"\n",
    "        f\"open_f1={test_metrics['open_token_f1']:.4f} \"\n",
    "        f\"(n={test_metrics['n']}, yesno={test_metrics['n_yesno']}, open={test_metrics['n_open']})\"\n",
    "    )\n",
    "\n",
    "    with open(os.path.join(OUTPUT_DIR, \"test_metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(test_metrics, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e642eb6e-fb16-4382-ae2a-68a1d9bcfdb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
