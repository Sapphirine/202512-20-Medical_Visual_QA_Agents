# Medical-ACE: Medical Visual Question Answering System

ğŸ¥ AI-powered medical image analysis system combining vision and language models.

*EECS6893 Big Data Analytics Final Project: A Multi-Agent System that Reads and Understands Medical Images at Scale*

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Activate conda environment
conda activate ACE

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure API Keys

Create `.env` file:

```bash
OPENAI_API_KEY=your_openai_api_key_here
LANGCHAIN_TRACING_V2=true
LANGCHAIN_PROJECT=Medical-ACE
```

### 3. Run the System

#### Option A: LangGraph Studio (Recommended)

```bash
langgraph dev --tunnel
```

Then visit the URL shown in terminal to use the visual Studio interface.

#### Option B: Command Line

```bash
# Single query
python main.py --mode single

# Batch processing
python main.py --mode batch

# Interactive mode
python main.py --mode interactive
```

#### Option C: Test Script

```bash
python test_agent.py
```

## ğŸ—ï¸ Architecture

```
Medical-ACE/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â””â”€â”€ medical_assistant_agent.py  # Main agent (GPT-4o + VQA tool)
â”‚   â””â”€â”€ tools/
â”‚       â””â”€â”€ medical_vqa_tool.py         # Medical VQA tool
â”œâ”€â”€ inference.py                         # Multimodal inference engine
â”œâ”€â”€ projector_epoch2.pt                 # Trained projector model
â”œâ”€â”€ main.py                             # CLI entry point
â””â”€â”€ langgraph.json                      # LangGraph configuration
```

## ğŸ”§ Components

### Agent
- **Model**: GPT-4o (OpenAI)
- **Type**: LangChain `create_agent`
- **Tools**: Medical VQA Tool

### Medical VQA Tool
- **Vision**: CLIP (openai/clip-vit-base-patch32)
- **Language**: TinyLlama (1.1B)
- **Projector**: Custom trained (epoch 2)

## ğŸ“Š Supported Image Types

- ğŸ”¬ Pathology slides
- ğŸ©» X-rays
- ğŸ§  CT/MRI scans
- ğŸ«€ Other medical imaging

## ğŸ’¡ Usage Examples

### Python API

```python
from src.agents import medical_assistant

# Query the agent
result = medical_assistant.invoke({
    "messages": [{"role": "user", "content": "Analyze image.png"}]
})

print(result["messages"][-1].content)
```

### LangGraph Studio

1. Start server: `langgraph dev --tunnel`
2. Open Studio in browser
3. Select `medical_assistant` agent
4. Chat with the agent about medical images

## ğŸ”¬ Testing

```bash
# Test the agent
python test_agent.py

# Test inference module directly
python inference.py image.png "What is shown?" projector_epoch2.pt
```

## âš™ï¸ Configuration

Edit `src/agents/medical_assistant_agent.py` to customize:
- Model selection
- System prompt
- Tools

Edit `langgraph.json` to add more agents.

## ğŸ“ Notes

âš ï¸ **Medical Disclaimer**: This system is for research/education only. AI analysis should be verified by qualified medical professionals.

## ğŸ”— Resources

- [LangChain Docs](https://docs.langchain.com)
- [LangGraph Studio](https://docs.langchain.com/langgraph/studio)
- [Project Documentation](docs/MEDICAL_ASSISTANT_GUIDE.md)

## ğŸ“„ License

MIT License
